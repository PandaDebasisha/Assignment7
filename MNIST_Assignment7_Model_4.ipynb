{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8kH16rnZ7wt_"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Target:**\n",
        "\n",
        "Stable model test accuracy to be > 99.4% over >7  Epochs.\n",
        "Result Retained at Last Epoch also.\n",
        "\n",
        "**Results:**\n",
        "1. Parameters: **7774**\n",
        "2. Best Train Accuracy: **99.41%**\n",
        "3. Best Test Accuracy: **99.44%**\n",
        "\n",
        "**Analysis:**\n",
        "1. Model Architecture is tuned with less Filter size to reduce  not only total Parameter also we can manage 28*28 image with less filters\n",
        "2. Learning Rate is Tuned with STepLR to acheive better consistent accuracy\n",
        "3. Augmentation is used like Color Jitter Random Rotaion for **+/-5%** and Normalization\n",
        "4. Best part of Model 1 is : Least gap and Costinency is observed between train and test accuracy \n",
        "5. Loss Covergence is Quite good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YtssFUKb-jqx"
      },
      "outputs": [],
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.Resize((28, 28)),\n",
        "                                       transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.RandomRotation((-5.0, 5.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4A84rlfDA23",
        "outputId": "2cc0a4ce-205a-4bdf-ffb7-753398ed571f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.11MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 120kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 923kB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8OLDR79DrHG",
        "outputId": "66a9b95a-92e4-4194-c6a4-878b0ac5942a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ]
        }
      ],
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7FXQlB9kH1ov"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "dropout_value = 0.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 26\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 24\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=12, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 24\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 10\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(14),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 8\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 6\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=14, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(14),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 6\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=6)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=14, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(10),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock8(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo"
      },
      "source": [
        "# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is.\n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5skB97zIJQQe",
        "outputId": "b1fe5230-279d-40c4-a500-49eb192cd239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in c:\\users\\91702\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]              90\n",
            "              ReLU-2           [-1, 10, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 10, 26, 26]              20\n",
            "           Dropout-4           [-1, 10, 26, 26]               0\n",
            "            Conv2d-5           [-1, 12, 24, 24]           1,080\n",
            "              ReLU-6           [-1, 12, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 12, 24, 24]              24\n",
            "           Dropout-8           [-1, 12, 24, 24]               0\n",
            "            Conv2d-9           [-1, 10, 24, 24]             120\n",
            "        MaxPool2d-10           [-1, 10, 12, 12]               0\n",
            "           Conv2d-11           [-1, 10, 10, 10]             900\n",
            "             ReLU-12           [-1, 10, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 10, 10, 10]              20\n",
            "          Dropout-14           [-1, 10, 10, 10]               0\n",
            "           Conv2d-15             [-1, 14, 8, 8]           1,260\n",
            "             ReLU-16             [-1, 14, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 14, 8, 8]              28\n",
            "          Dropout-18             [-1, 14, 8, 8]               0\n",
            "           Conv2d-19             [-1, 16, 6, 6]           2,016\n",
            "             ReLU-20             [-1, 16, 6, 6]               0\n",
            "      BatchNorm2d-21             [-1, 16, 6, 6]              32\n",
            "          Dropout-22             [-1, 16, 6, 6]               0\n",
            "           Conv2d-23             [-1, 14, 6, 6]           2,016\n",
            "             ReLU-24             [-1, 14, 6, 6]               0\n",
            "      BatchNorm2d-25             [-1, 14, 6, 6]              28\n",
            "          Dropout-26             [-1, 14, 6, 6]               0\n",
            "        AvgPool2d-27             [-1, 14, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             140\n",
            "================================================================\n",
            "Total params: 7,774\n",
            "Trainable params: 7,774\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.56\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.60\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs.\n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fbkF2nN_LYIb"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE5Le6FYHhc8",
        "outputId": "a0178258-31c3-4644-b93f-a93181ad1760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.10382454842329025 Batch_id=468 Accuracy=93.60: 100%|██████████| 469/469 [00:06<00:00, 72.35it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0562, Accuracy: 9836/10000 (98.36%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.013904877007007599 Batch_id=468 Accuracy=98.33: 100%|██████████| 469/469 [00:06<00:00, 72.64it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0421, Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.06856909394264221 Batch_id=468 Accuracy=98.69: 100%|██████████| 469/469 [00:06<00:00, 72.39it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.019036216661334038 Batch_id=468 Accuracy=98.87: 100%|██████████| 469/469 [00:06<00:00, 72.33it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0288, Accuracy: 9912/10000 (99.12%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.04306738078594208 Batch_id=468 Accuracy=99.20: 100%|██████████| 469/469 [00:06<00:00, 70.20it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 9939/10000 (99.39%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.009497318416833878 Batch_id=468 Accuracy=99.31: 100%|██████████| 469/469 [00:07<00:00, 62.61it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 9939/10000 (99.39%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.02535936050117016 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:07<00:00, 63.02it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0210, Accuracy: 9941/10000 (99.41%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.02803097479045391 Batch_id=468 Accuracy=99.36: 100%|██████████| 469/469 [00:07<00:00, 63.39it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0204, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.02037068083882332 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:07<00:00, 63.54it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9939/10000 (99.39%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.004072252660989761 Batch_id=468 Accuracy=99.39: 100%|██████████| 469/469 [00:07<00:00, 62.67it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 9943/10000 (99.43%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.008614900521934032 Batch_id=468 Accuracy=99.41: 100%|██████████| 469/469 [00:07<00:00, 62.15it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.010816306807100773 Batch_id=468 Accuracy=99.41: 100%|██████████| 469/469 [00:07<00:00, 62.99it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0197, Accuracy: 9944/10000 (99.44%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.00832297932356596 Batch_id=468 Accuracy=99.39: 100%|██████████| 469/469 [00:07<00:00, 63.70it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0199, Accuracy: 9943/10000 (99.43%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.007128103170543909 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:07<00:00, 63.44it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9943/10000 (99.43%)\n",
            "\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.02005930431187153 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:07<00:00, 63.73it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9942/10000 (99.42%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "import math\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9,nesterov=True,\n",
        "                         weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "#scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.02, epochs=15, steps_per_epoch=len(train_loader))\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq"
      },
      "source": [
        "# Let's Train and test our model\n",
        "\n",
        "This time let's add a scheduler for out LR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "87RaqGSEOWDe",
        "outputId": "5d317b93-077b-4aa8-f340-439340a1bb27"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m axs[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m axs[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(train_acc[\u001b[38;5;241m4000\u001b[39m:])\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:491\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    489\u001b[0m     y \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mindex_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mxaxis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     axes\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mupdate_units(x)\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\matplotlib\\cbook.py:1666\u001b[0m, in \u001b[0;36mindex_of\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1666\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1668\u001b[0m     \u001b[38;5;66;03m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\matplotlib\\cbook.py:1358\u001b[0m, in \u001b[0;36m_check_1d\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m-> 1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[1;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\91702\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x1000 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "axs[0, 0].plot(train_losses)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[1, 0].plot(train_acc[4000:])\n",
        "axs[1, 0].set_title(\"Training Accuracy\")\n",
        "axs[0, 1].plot(test_losses)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 1].plot(test_acc)\n",
        "axs[1, 1].set_title(\"Test Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjO3RK9UEnvF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
